{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrackMateAnalyzer\n",
    "\n",
    "This script is intened to batch extract, compile, and analyze data from TrackMate output files. \n",
    "\n",
    "### Directions\n",
    "1. Create a folder for each condition of data (ex. wildtype cells vs mutant cells)\n",
    "2. Run TrackMate on Fiji/ImageJ, on the Display Options page, select 'Tracks' at the bottom. In the pop up window 'Track tables', select 'Tracks', then export to CSV. Save the CSV into a subfolder within the appropriate condition folder (keep the name as the default 'export.csv'). \n",
    "\n",
    "    Example directory: C:\\Users\\username\\Desktop\\ExperimentReplicates\\Replicate1\\Condition1\\TrackMateFolder1\\export.csv\\\n",
    "\n",
    "    If you only have one replicate of the experiment: C:\\Users\\username\\Desktop\\TrackMateExperiment\\Condition1\\TrackMateFolder1\\export.csv\\\n",
    "\n",
    "3. Run the following script, it will ask for several prompts:\n",
    "\n",
    "    a. Enter the number of conditions to be analyzed (statistical analysis necessitates at least 2 conditions)\n",
    "\n",
    "    b. Enter the name of each condition to be analyzed (ex. wildtype and mutant), it is essential these names match the name of the condition folder and if you are analyzing multiple replicates, the condition folders have the same names across different replicate folders\n",
    "\n",
    "    c. Enter the minimum track duration (sec) you would like (this will filter out all tracks under the number you enter, so if you enter 10, all the data will be from tracks that had a duration of at least 10 seconds)\n",
    "\n",
    "    d. Enter 'Y' if you would like to analyze more than one replicate (a replicate is a single iteration of an experiment, analyzing multiple replicates would mean you would like to combine the data across multiple iterations of an experiment), 'N' if you would like to analyze a single replicate\n",
    "\n",
    "    e. Enter 'Y' if you would like to normalize the conditions to the mean of condition1 (this is the first condition name you enter, I recommend entering your control or wt condition first), the normalization occurs by: conditionX_value*(conditionX_mean/condition1_mean). Enter 'N' if you do not want to normalize the data.\n",
    "\n",
    "    f. A window will pop up to ask for the directory to analyze. If you are analyzing multiple replicates, select the directory that contains the folders for each replicate of the experiment. If you are analyzing a single replicate, choose the folder of the single replicate.\n",
    "\n",
    "        Multiple Replicates Example Directory: C:\\Users\\username\\Desktop\\ExperimentReplicates\n",
    "\n",
    "        Single Replicate Example Directory: C:\\Users\\username\\Desktop\\ExperimentReplicates\\Replicate1 \n",
    "    \n",
    "\n",
    "### Troubleshooting Tips\n",
    "1. Confirm that all your trackmate files are called 'export.csv' and they are each located in an individual folder within each condition folder\n",
    "2. Confirm you entering the correct condition names and that these match the name of the condition folders exactly, and these are the same condition names across multiple folders\n",
    "3. If you are analyzing multiple replicates, make sure the only folders in your chosen directory are the replicate folders (aside from trackmate_results folder if have already run the script)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tkinter import filedialog\n",
    "import tkinter as tk\n",
    "from scipy.stats import ttest_ind, f_oneway\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "import sys\n",
    "import re \n",
    "\n",
    "# # Function to clean sheet names\n",
    "# def clean_sheet_name(title):\n",
    "#     # Remove invalid characters from the title\n",
    "#     cleaned_title = re.sub(r'[\\\\[\\]:*?/]', '_', title)\n",
    "#     # Truncate the sheet name if it exceeds 31 characters\n",
    "#     return cleaned_title[:31]\n",
    "\n",
    "\n",
    "# Ask the user for the number of conditions\n",
    "num_conditions = int(input(\"Enter the number of conditions (must be over 1): \"))\n",
    "\n",
    "if num_conditions == 1:\n",
    "    print('Cannot perform statistical analysis on one condition')\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# Ask the user for the name of each condition folder\n",
    "condition_names = [input(f\"Enter the name of condition {i+1}: \") for i in range(num_conditions)]\n",
    "\n",
    "\n",
    "# Ask the user for track duration to filter out\n",
    "track_duration_input = int(input(\"Enter the minimum track duration (sec) you would like (Please enter whole number): \"))\n",
    "\n",
    "# Ask the user if they have one replicate or multiple replicates of data\n",
    "replicate_input = input(\"Do you have more than one replicate? (Enter 'y/n'): \")\n",
    "replicates = replicate_input.lower() in ['y', 'yes']\n",
    "\n",
    "normalize_input = input('Do you want to normalize all conditions to the mean of condition 1? (Enter y/n): ')\n",
    "normalized = normalize_input.lower() in ['y', 'yes']\n",
    "\n",
    "# Create a Tkinter root window\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Hide the root window\n",
    "\n",
    "if replicates:\n",
    "    main_directory = filedialog.askdirectory(title=\"Select Directory Containing Replicate Folders\")\n",
    "else:\n",
    "    main_directory = filedialog.askdirectory(title=\"Select Directory Containing Condition Folders\")\n",
    "\n",
    "print(main_directory)\n",
    "\n",
    "# Check if a directory was selected\n",
    "if not main_directory:\n",
    "    print(\"No directory selected. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "# All metrics from trackmate export.csv files\n",
    "column_titles = ['NUMBER_SPOTS', 'NUMBER_GAPS', 'NUMBER_SPLITS', 'NUMBER_MERGES',\n",
    "                 'NUMBER_COMPLEX', 'LONGEST_GAP', 'TRACK_START', 'TRACK_STOP', \n",
    "                 'TRACK_DISPLACEMENT', 'TRACK_DURATION', 'TRACK_MEAN_SPEED', \n",
    "                 'TRACK_MAX_SPEED', 'TRACK_MIN_SPEED', 'TRACK_MEDIAN_SPEED', \n",
    "                 'TRACK_STD_SPEED', 'TRACK_MEAN_QUALITY', 'TOTAL_DISTANCE_TRAVELED', \n",
    "                 'MAX_DISTANCE_TRAVELED', 'CONFINEMENT_RATIO', 'MEAN_STRAIGHT_LINE_SPEED',\n",
    "                 'LINEARITY_OF_FORWARD_PROGRESSION', 'MEAN_DIRECTIONAL_CHANGE_RATE',\n",
    "                 'TRACK_X_LOCATION', 'TRACK_Y_LOCATION', 'TRACK_Z_LOCATION']\n",
    "\n",
    "# Title to be used for histogram plots\n",
    "subtitles = [ 'TRACK_DISPLACEMENT', 'TRACK_DURATION', 'TRACK_MEAN_SPEED', \n",
    "                 'TRACK_MAX_SPEED', 'TRACK_MIN_SPEED', 'TRACK_MEDIAN_SPEED', \n",
    "                 'TRACK_STD_SPEED', 'TRACK_MEAN_QUALITY', 'TOTAL_DISTANCE_TRAVELED', \n",
    "                 'MAX_DISTANCE_TRAVELED', 'CONFINEMENT_RATIO', 'MEAN_STRAIGHT_LINE_SPEED',\n",
    "                 'LINEARITY_OF_FORWARD_PROGRESSION', 'MEAN_DIRECTIONAL_CHANGE_RATE']\n",
    "\n",
    "\n",
    "# Create a dictionary to hold DataFrames for each condition\n",
    "df_dict = {condition: {title: pd.DataFrame() for title in column_titles} for condition in condition_names}\n",
    "\n",
    "# Check if all required folders exist, if not, exit the program\n",
    "if replicates:\n",
    "    replicate_folders = os.listdir(main_directory)\n",
    "    print('Replicate folders: ', replicate_folders)\n",
    "    condition_paths = []\n",
    "    for folder in replicate_folders:\n",
    "        if folder not in ['trackmate_results']:\n",
    "            condition_paths.append([os.path.join(main_directory, folder, condition) for condition in condition_names])\n",
    "            print('Condition paths for multiple replicates: ', condition_paths[-1], condition_paths)\n",
    "            if not all(os.path.exists(folder) for folder in condition_paths[-1]):\n",
    "                print(\"One or more required folders are missing.\")\n",
    "                sys.exit()\n",
    "else:\n",
    "    condition_paths = [os.path.join(main_directory, folder) for folder in condition_names]\n",
    "    print(condition_names)\n",
    "    print('Condition paths: ', condition_paths)\n",
    "    if not all(os.path.exists(folder) for folder in condition_paths):\n",
    "        print(\"One or more required folders are missing.\")\n",
    "        sys.exit()\n",
    "\n",
    "#Data processing for multiple replicates\n",
    "if replicates:\n",
    "    print(\"Processing multiple replicates...\")\n",
    "    # Iterate over each replicate folder\n",
    "    for replicate_folder in os.listdir(main_directory):\n",
    "        replicate_path = os.path.join(main_directory, replicate_folder)\n",
    "\n",
    "        if not os.path.isdir(replicate_path): #Ignore if not a folder\n",
    "            continue\n",
    "        if replicate_folder=='trackmate_results': #Ignore if trackmate_results folder\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing replicate: {replicate_folder}\")\n",
    "\n",
    "        # Iterate over each condition\n",
    "        for condition in condition_names:\n",
    "            condition_path = os.path.join(replicate_path, condition)\n",
    "            \n",
    "            if not os.path.isdir(condition_path):\n",
    "                print(f\"Condition folder '{condition}' does not exist in replicate '{replicate_folder}'. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Processing condition: {condition}\")\n",
    "            \n",
    "            # Initialize a dictionary to hold DataFrames for each column\n",
    "            column_dfs = {}\n",
    "            \n",
    "            # Iterate over folders in the condition directory\n",
    "            for folder_idx, folder_name in enumerate(os.listdir(condition_path)):\n",
    "                folder_path = os.path.join(condition_path, folder_name)\n",
    "                \n",
    "                if not os.path.isdir(folder_path):\n",
    "                    continue\n",
    "                \n",
    "                # Check if export.csv exists in the folder\n",
    "                export_csv_path = os.path.join(folder_path, 'export.csv')\n",
    "                if not os.path.exists(export_csv_path):\n",
    "                    print(f\"export.csv not found in folder '{folder_name}'. Skipping...\")\n",
    "                    continue\n",
    "                \n",
    "                # Read the CSV file into a pandas DataFrame\n",
    "                df = pd.read_csv(export_csv_path)\n",
    "\n",
    "                # Convert columns to numeric values\n",
    "                df[column_titles] = df[column_titles].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "                # Filter rows based on conditions\n",
    "                filtered_rows = df.loc[(~df[column_titles].isna().any(axis=1)) & (df['TRACK_DURATION'] >= track_duration_input)]\n",
    "\n",
    "                # Add filtered rows to the dictionary\n",
    "                for column in column_titles:\n",
    "                    if not filtered_rows[column].isna().all():\n",
    "                        key = f\"{main_directory}_{folder_name}_data\"\n",
    "                        if key not in column_dfs:\n",
    "                            column_dfs[key] = {}\n",
    "                        if column not in column_dfs[key]:\n",
    "                            column_dfs[key][column] = []\n",
    "                        column_dfs[key][column].extend(filtered_rows[column].reset_index(drop=True))\n",
    "\n",
    "            # Combine data for each condition and column into a single DataFrame\n",
    "            for key, data_dict in column_dfs.items():\n",
    "                condition_df = pd.DataFrame(data_dict)\n",
    "                if condition not in df_dict:\n",
    "                    df_dict[condition] = condition_df\n",
    "                else:\n",
    "                    for column, data in data_dict.items():\n",
    "                        if column not in df_dict[condition]:\n",
    "                            df_dict[condition][column] = data\n",
    "                        else:\n",
    "                            df_dict[condition][column] = pd.concat([df_dict[condition][column], pd.Series(data)], axis=1)\n",
    "        \n",
    "\n",
    "#Data processing one replicate of data\n",
    "else:\n",
    "    print(\"Processing single replicate...\")\n",
    "\n",
    "\n",
    "    # Iterate over each condition\n",
    "    for condition in condition_names:\n",
    "        condition_path = os.path.join(main_directory, condition)\n",
    "        \n",
    "        if not os.path.isdir(condition_path):\n",
    "            print(f\"Condition folder '{condition}' does not exist in replicate '{main_directory}'. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Processing condition: {condition}\")\n",
    "        \n",
    "        # Initialize a dictionary to hold DataFrames for each column\n",
    "        column_dfs = {}\n",
    "        \n",
    "        # Iterate over folders in the condition directory\n",
    "        for folder_idx, folder_name in enumerate(os.listdir(condition_path)):\n",
    "            folder_path = os.path.join(condition_path, folder_name)\n",
    "            \n",
    "            if not os.path.isdir(folder_path):\n",
    "                continue\n",
    "            \n",
    "            # Check if export.csv exists in the folder\n",
    "            export_csv_path = os.path.join(folder_path, 'export.csv')\n",
    "            if not os.path.exists(export_csv_path):\n",
    "                print(f\"export.csv not found in folder '{folder_name}'. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Read the CSV file into a pandas DataFrame\n",
    "            df = pd.read_csv(export_csv_path)\n",
    "\n",
    "            # Convert columns to numeric values\n",
    "            df[column_titles] = df[column_titles].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "            # Filter rows based on conditions\n",
    "            filtered_rows = df.loc[(~df[column_titles].isna().any(axis=1)) & (df['TRACK_DURATION'] >= track_duration_input)]\n",
    "\n",
    "            # Add filtered rows to the dictionary\n",
    "            for column in column_titles:\n",
    "                if not filtered_rows[column].isna().all():\n",
    "                    key = f\"{main_directory}_{folder_name}_data\"\n",
    "                    if key not in column_dfs:\n",
    "                        column_dfs[key] = {}\n",
    "                    if column not in column_dfs[key]:\n",
    "                        column_dfs[key][column] = []\n",
    "                    column_dfs[key][column].extend(filtered_rows[column].reset_index(drop=True))\n",
    "\n",
    "        # Combine data for each condition and column into a single DataFrame\n",
    "        for key, data_dict in column_dfs.items():\n",
    "            condition_df = pd.DataFrame(data_dict)\n",
    "            if condition not in df_dict:\n",
    "                df_dict[condition] = condition_df\n",
    "            else:\n",
    "                for column, data in data_dict.items():\n",
    "                    if column not in df_dict[condition]:\n",
    "                        df_dict[condition][column] = data\n",
    "                    else:\n",
    "                        df_dict[condition][column] = pd.concat([df_dict[condition][column], pd.Series(data)], axis=1)\n",
    "\n",
    "\n",
    "# Create the trackmate_results folder in the chosen directory\n",
    "trackmate_results_folder = os.path.join(main_directory, 'trackmate_results')\n",
    "if not os.path.exists(trackmate_results_folder):\n",
    "    os.makedirs(trackmate_results_folder)\n",
    "\n",
    "# Create a folder for graphs\n",
    "graphs_folder = os.path.join(trackmate_results_folder, 'graphs')\n",
    "if not os.path.exists(graphs_folder):\n",
    "    os.makedirs(graphs_folder)\n",
    "\n",
    "# Save the raw DataFrames to separate Excel files for each condition with each metric in a different sheet\n",
    "for condition, df_group in df_dict.items():\n",
    "    with pd.ExcelWriter(os.path.join(trackmate_results_folder, f\"{condition}_compiled_raw.xlsx\")) as writer:\n",
    "        for title, df in df_group.items():\n",
    "\n",
    "            # Truncate the sheet name if it exceeds 31 characters\n",
    "            sheet_name = title[:31] if len(title) > 31 else title\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "'''\n",
    "Calculate Mean Values\n",
    "'''\n",
    "# Create a dictionary to hold DataFrames for mean values of each condition\n",
    "mean_df_dict = {}\n",
    "\n",
    "# Iterate over each metric title\n",
    "for title in column_titles:\n",
    "    # Create an empty DataFrame for the current metric\n",
    "    mean_df_dict[title] = pd.DataFrame(index=range(500))  # Predefined number of rows\n",
    "    \n",
    "    reference_condition = condition_names[0]  # Assuming the first condition is the reference\n",
    "\n",
    "    # Iterate over each condition\n",
    "    for condition, df_group in df_dict.items():\n",
    "        \n",
    "        # Get the DataFrame for the current condition and metric\n",
    "        df = df_group.get(title)\n",
    "        \n",
    "        # Check if the DataFrame exists for the current condition and metric\n",
    "        if df is not None:\n",
    "            # Calculate mean values for each column, skipping missing values\n",
    "            mean_values = df.mean(skipna=True)\n",
    "            \n",
    "            # Convert mean values to list\n",
    "            mean_values_list = mean_values.tolist()\n",
    "            \n",
    "            # Add NaN values to make the list length equal to 500\n",
    "            mean_values_list.extend([np.nan] * (500 - len(mean_values_list)))\n",
    "            \n",
    "            # Fill in the mean values into the DataFrame\n",
    "            mean_df_dict[title][condition] = mean_values_list\n",
    "        else:\n",
    "            print(f\"    No data found for condition {condition} and metric {title}\")\n",
    "\n",
    "    # Normalize values to the mean of the reference condition\n",
    "    if normalized:\n",
    "        # Extract the mean values of the reference condition\n",
    "        scale_factor = mean_df_dict[title][reference_condition].mean()\n",
    "\n",
    "        for condition in condition_names:\n",
    "            if condition != reference_condition:\n",
    "                # Normalize the current condition by dividing by the scale factor\n",
    "                mean_df_dict[title][condition] = mean_df_dict[title][condition]*(mean_df_dict[title][condition].mean() / scale_factor)\n",
    "\n",
    "# Save the mean DataFrames to a single Excel file with each metric in a separate sheet\n",
    "if normalized:\n",
    "    mean_excel_path = os.path.join(trackmate_results_folder, 'normalized_mean_values.xlsx')\n",
    "else:\n",
    "    mean_excel_path = os.path.join(trackmate_results_folder, 'mean_values.xlsx')\n",
    "with pd.ExcelWriter(mean_excel_path) as writer:\n",
    "    for title, mean_df in mean_df_dict.items():\n",
    "        # Truncate the title if it exceeds 31 characters\n",
    "        sheet_name = title[:31]\n",
    "        \n",
    "        mean_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(\"Mean value calculation and saving completed.\")\n",
    "\n",
    "# Initialize a dictionary to store t-test and ANOVA results for each mean dataframe\n",
    "stats_results_dict = {title: [] for title in mean_df_dict.keys()}\n",
    "\n",
    "# Perform t-tests and ANOVA for each mean dataframe\n",
    "for title, mean_df in mean_df_dict.items():\n",
    "    # Perform t-tests for each pair of conditions\n",
    "    for i in range(len(condition_names)):\n",
    "        for j in range(i+1, len(condition_names)):\n",
    "            condition1 = condition_names[i]\n",
    "            condition2 = condition_names[j]\n",
    "            \n",
    "            # Perform t-test\n",
    "            t_statistic, p_value = ttest_ind(mean_df[condition1].dropna(), mean_df[condition2].dropna())\n",
    "            \n",
    "            # Determine significance level\n",
    "            if p_value < 0.0001:\n",
    "                significance = '****'\n",
    "            elif p_value < 0.001:\n",
    "                significance = '***'\n",
    "            elif p_value < 0.01:\n",
    "                significance = '**'\n",
    "            elif p_value < 0.05:\n",
    "                significance = '*'\n",
    "            else:\n",
    "                significance = 'ns'\n",
    "            \n",
    "            # Append t-test results to the dictionary\n",
    "            stats_results_dict[title].append({'Comparison': f\"T-test between {condition1} and {condition2} ({title}):\",\n",
    "                                            'T-Statistic': t_statistic,\n",
    "                                            'P-Value': p_value,\n",
    "                                            'Significance': significance})\n",
    "    \n",
    "    # Perform ANOVA if there are 2 or more conditions\n",
    "    if len(condition_names) >= 2:\n",
    "        anova_samples = [mean_df[condition].dropna() for condition in condition_names]\n",
    "        anova_result = f_oneway(*anova_samples)\n",
    "        \n",
    "        # Determine ANOVA significance level\n",
    "        if anova_result.pvalue < 0.0001:\n",
    "            anova_significance = '****'\n",
    "        elif anova_result.pvalue < 0.001:\n",
    "            anova_significance = '***'\n",
    "        elif anova_result.pvalue < 0.01:\n",
    "            anova_significance = '**'\n",
    "        elif anova_result.pvalue < 0.05:\n",
    "            anova_significance = '*'\n",
    "        else:\n",
    "            anova_significance = 'ns'\n",
    "        \n",
    "        # Append ANOVA results to the dictionary\n",
    "        stats_results_dict[title].append({'Comparison': 'ANOVA:',\n",
    "                                        'F-Statistic': anova_result.statistic,\n",
    "                                        'P-Value': anova_result.pvalue,\n",
    "                                        'Significance': anova_significance})\n",
    "\n",
    "# Save the statistical test results to an Excel file\n",
    "stats_excel_path = os.path.join(trackmate_results_folder, 'statistical_test_results.xlsx')\n",
    "with pd.ExcelWriter(stats_excel_path) as writer:\n",
    "    for title, stats_results in stats_results_dict.items():\n",
    "        # Truncate the title if it exceeds 31 characters\n",
    "        sheet_name = title[:31]\n",
    "        \n",
    "        stats_results_df = pd.DataFrame(stats_results)\n",
    "        stats_results_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(\"Statistical test results saved.\")\n",
    "\n",
    "\n",
    "# Function to calculate the number of bins using different methods (can edit to choose different methods)\n",
    "def calculate_bins(data, method='sqrt'):\n",
    "    data = data[~np.isnan(data)]  # Remove NaN values\n",
    "    n = len(data)\n",
    "    if n == 0:\n",
    "        return 1  # Default to 1 bin if no data left after removing NaNs\n",
    "    if method == 'sqrt':\n",
    "        return int(np.sqrt(n))\n",
    "    elif method == 'sturges':\n",
    "        return int(np.log2(n) + 1)\n",
    "    elif method == 'rice':\n",
    "        return int(2 * (n ** (1 / 3)))\n",
    "    elif method == 'fd':  # Freedman-Diaconis\n",
    "        q25, q75 = np.percentile(data, [25, 75])\n",
    "        bin_width = 2 * (q75 - q25) / (n ** (1 / 3))\n",
    "        if bin_width == 0 or np.isnan(bin_width):\n",
    "            return 1  # Return 1 bin if bin_width is 0 or NaN\n",
    "        return max(1, int((data.max() - data.min()) / bin_width))  # Ensure at least 1 bin\n",
    "    else:\n",
    "        raise ValueError(\"Unknown method\")\n",
    "    \n",
    "\n",
    "# Perform violin plots for each metric\n",
    "for title, mean_df in mean_df_dict.items():\n",
    "    # Plot settings\n",
    "\n",
    "    dots_palette = []\n",
    "\n",
    "    for condition in range(num_conditions):\n",
    "        dots_palette.append('white')\n",
    "\n",
    "    magma_palette = sns.color_palette(\"magma\", n_colors=num_conditions)\n",
    "\n",
    "    lighter_palette = []\n",
    "\n",
    "    for color in magma_palette:\n",
    "        new_color = tuple(min(component + 0.4 * (1 - component), 1) for component in color)\n",
    "        lighter_palette.append(new_color)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    sns.violinplot(data=mean_df, bw_method=0.3, linewidth=3, palette=lighter_palette, inner='quartile', inner_kws={'color': 'black', 'linewidth': 4}, linecolor='black')\n",
    "    sns.stripplot(data=mean_df, size=15, palette=dots_palette, linewidth=2, edgecolor='black')\n",
    "                                                              \n",
    "    label_map = {\n",
    "        \"LONGEST_GAP\": \"Longest gap (#Frames)\",\n",
    "        \"TRACK_DURATION\": \"Duration (sec)\",\n",
    "        \"TRACK_START\": \"Track start (sec)\",\n",
    "        \"TRACK_STOP\": \"Track stop (sec)\",\n",
    "        \"TRACK_DISPLACEMENT\": \"Track displacement (micron)\",\n",
    "        \"TRACK_X_LOCATION\": \"Track mean position (micron)\",\n",
    "        \"TRACK_Y_LOCATION\": \"Track mean position (micron)\",\n",
    "        \"TRACK_Z_LOCATION\": \"Track mean position (micron)\",\n",
    "        \"TRACK_MEAN_SPEED\": \"Mean speed (micron/sec)\",\n",
    "        \"TRACK_MAX_SPEED\": \"Max speed (micron/sec)\",\n",
    "        \"TRACK_MIN_SPEED\": \"Min speed (micron/sec)\",\n",
    "        \"TRACK_MEDIAN_SPEED\": \"Median speed (micron/sec)\",\n",
    "        \"TRACK_STD_SPEED\": \"Std speed (micron/sec)\",\n",
    "        \"TRACK_MEAN_QUALITY\": \"Mean quality (quality)\",\n",
    "        \"TOTAL_DISTANCE_TRAVELED\": \"Total distance traveled (micron)\",\n",
    "        \"MAX_DISTANCE_TRAVELED\": \"Max distance traveled (micron)\",\n",
    "        \"CONFINEMENT_RATIO\": \"Confinement ratio\",\n",
    "        \"MEAN_STRAIGHT_LINE_SPEED\": \"Mean straight line speed (micron/sec)\",\n",
    "        \"LINEARITY_OF_FORWARD_PROGRESSION\": \"Linearity of forward progression\",\n",
    "        \"MEAN_DIRECTIONAL_CHANGE_RATE\": \"Mean directional change rate (rad/sec)\",\n",
    "        \"NUMBER_SPOTS\": \"Number of spots in track\",\n",
    "        \"NUMBER_GAPS\": \"Number of gaps\",\n",
    "        \"NUMBER_SPLITS\": \"Number of split events\",\n",
    "        \"NUMBER_MERGES\": \"Number of merge events\",\n",
    "        \"NUMBER_COMPLEX\": \"Number of complex points\",\n",
    "        \"X\": \"Mean X position\",\n",
    "        \"Z\": \"Mean Z position\",\n",
    "        \"Y\": \"Mean Y position\"\n",
    "    }\n",
    "\n",
    "    # Default label if no specific format is defined\n",
    "    ylabel = label_map.get(title, \"Y Label\")\n",
    "\n",
    "\n",
    "    plt.ylabel(ylabel, fontsize=18, fontweight='bold')\n",
    "    plt.xticks(rotation=45, fontsize=14, fontweight='bold')\n",
    "    plt.title(title, fontsize=22, fontweight='bold')   \n",
    "    plt.tick_params(axis='y', labelsize=12) \n",
    "    violin_plot_path = os.path.join(graphs_folder, f\"{title}_violin_plot.pdf\")\n",
    "    plt.savefig(violin_plot_path)\n",
    "    plt.savefig(violin_plot_path.replace(\".pdf\", \".png\")) \n",
    "    plt.close()\n",
    "\n",
    "    # Histogram plots\n",
    "    if title in subtitles:\n",
    "        data = mean_df_dict[title].values\n",
    "        bins = calculate_bins(data, method='rice')\n",
    "\n",
    "        sns.histplot(data=mean_df_dict[title], bins=bins, kde=True, shrink=0.8, multiple='dodge', palette=magma_palette, alpha=0.5, legend=True)\n",
    "        plt.ylabel('Frequency', fontsize=18, fontweight='bold')\n",
    "        plt.xlabel(ylabel, fontsize=18, fontweight='bold')  # Use ylabel for xlabel\n",
    "        plt.xticks(rotation=45, fontsize=16, fontweight='bold')\n",
    "        plt.title(title, fontsize=22, fontweight='bold')   \n",
    "        plt.tick_params(axis='y', labelsize=12) \n",
    "\n",
    "        # Adjust layout parameters\n",
    "        plt.tight_layout(pad=3.0)\n",
    "        \n",
    "        hist_plot_path = os.path.join(graphs_folder, f\"{title}_hist_plot.pdf\")\n",
    "        plt.savefig(hist_plot_path)\n",
    "        plt.savefig(hist_plot_path.replace(\".pdf\", \".png\"))\n",
    "        plt.close()\n",
    "\n",
    "  \n",
    "print(\"Plots saved.\")\n",
    "\n",
    "def imagegrid(title, imagename, num_cols=5, directory=graphs_folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(imagename):\n",
    "            img_path = os.path.join(directory, filename)\n",
    "            img = Image.open(img_path)\n",
    "            images.append(img)\n",
    "    \n",
    "    num_images = len(images)\n",
    "    if num_images == 0:\n",
    "        print(f\"No images found with the suffix '{imagename}' in directory '{directory}'\")\n",
    "        return\n",
    "    \n",
    "    num_rows = math.ceil(num_images / num_cols)\n",
    "    \n",
    "    # Calculate figure size based on the number of columns\n",
    "    fig_width = 60\n",
    "    fig_height = fig_width / num_cols * num_rows\n",
    "    \n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(fig_width, fig_height))\n",
    "    fig.subplots_adjust(hspace=.2, wspace=0.15)\n",
    "    \n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        if i < num_images:\n",
    "            img = images[i]\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_aspect('auto')  # Set aspect ratio to 'auto' for all images\n",
    "        else:\n",
    "            ax.axis('off')  # Hide the subplot if there are fewer images than subplots\n",
    "    \n",
    "    fig.suptitle(title, fontsize=70, wrap=True)  # Enable text wrapping for the title\n",
    "    save_path = os.path.join(directory, title + \".pdf\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print('Grid saved')\n",
    "\n",
    "\n",
    "imagegrid(\"Violin Plots\", \"violin_plot.png\") #make grid of violin plots\n",
    "imagegrid(\"Histogram Plots\", \"hist_plot.png\", num_cols=7) #make grid of histogram plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
